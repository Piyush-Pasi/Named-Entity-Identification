{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.8/site-packages (1.13.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (20.9)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.20.1)\n",
      "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.0.19)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (3.7.4.post0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (1.2.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: dill in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/anaconda3/lib/python3.8/site-packages (from datasets) (2021.10.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from packaging->datasets) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.8/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->datasets) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/Users/param/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 150.44it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer #use TF IDF transformer to change text vector created by count vectorizer\n",
    "from sklearn.svm import SVC# Support Vector Machine\n",
    "from sklearn.metrics import *\n",
    "import re\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "from string import punctuation\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/param/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/param/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/param/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/param/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import conlltags2tree, tree2conlltags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and vecotrizing using Tf-Idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/param/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-2ff79afbbe477ce3.arrow\n",
      "Loading cached processed dataset at /Users/param/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-3c067e784a57e25f.arrow\n",
      "Loading cached processed dataset at /Users/param/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-34919c5cb479a53e.arrow\n"
     ]
    }
   ],
   "source": [
    "NEI_dataset = dataset.map(lambda example: {'ner_tags': [1 if x > 0 else 0 for x in example[\"ner_tags\"] ]})#{'sentence1': 'My sentence: ' + example['sentence1']\n",
    "Tfidf_vect = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "Tfidf_vect.fit([word for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "Train_X_Tfidf = Tfidf_vect.transform([word for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist])\n",
    "Test_X_Tfidf = Tfidf_vect.transform([word for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "Test_Y = [word for sublist in NEI_dataset[\"test\"][\"ner_tags\"]  for word in sublist]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model using Tf-Idf Sparse Matrix representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.56519866480025\n"
     ]
    }
   ],
   "source": [
    "# Classifier - SVM\n",
    "# Vectorizer - Tf-IDF\n",
    "# Kernel - rbf\n",
    "# Fit the training dataset on the classifier\n",
    "SVM_tf_idf_rbf = SVC(kernel='rbf')\n",
    "SVM_tf_idf_rbf.fit(Train_X_Tfidf,[word for sublist in NEI_dataset[\"train\"][\"ner_tags\"]  for word in sublist])\n",
    "# Predict the labels on test dataset\n",
    "predictions_SVM_tf_idf_rbf = SVM_tf_idf_rbf.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_tf_idf_rbf, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing and vecotrizing using word embedding vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz',\n",
    "binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = []\n",
    "test_embeddings = []\n",
    "\n",
    "train_words = [word for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]#+[word for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist]\n",
    "test_words = [word for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist]\n",
    "count = 0\n",
    "for word in train_words:\n",
    "    if word in model_w2v:\n",
    "        train_embeddings.append(model_w2v.get_vector(word))\n",
    "    else:\n",
    "        # print(\"OHH NOO!!!!\")\n",
    "        count +=1\n",
    "        train_embeddings.append(np.zeros(300))\n",
    "\n",
    "for word in test_words:\n",
    "    if word in model_w2v:\n",
    "        test_embeddings.append(model_w2v.get_vector(word))\n",
    "    else:\n",
    "        # print(\"OHH NOO!!!!\")\n",
    "        # count +=1\n",
    "        test_embeddings.append(np.zeros(300))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM model using Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  97.45235275115753\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM_embedding_rbf = SVC(kernel='rbf')\n",
    "SVM_embedding_rbf.fit(train_embeddings,[word for sublist in NEI_dataset[\"train\"][\"ner_tags\"]  for word in sublist])\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_embedding_rbf = SVM_embedding_rbf.predict(test_embeddings)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_embedding_rbf, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  93.56519866480025\n",
      "SVM Precision Score ->  65.81607495069034\n",
      "SVM Recall Score ->  96.12891609650703\n",
      "SVM F1 Score ->  78.13551880579541\n"
     ]
    }
   ],
   "source": [
    "# Results for SVM model trained on Tf-Idf representations\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Precision Score -> \",precision_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM Recall Score -> \",recall_score(predictions_SVM, Test_Y)*100)\n",
    "print(\"SVM F1 Score -> \",f1_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  97.45235275115753\n",
      "SVM Precision Score ->  90.53254437869822\n",
      "SVM Recall Score ->  94.6513725995618\n",
      "SVM F1 Score ->  92.5461533614769\n"
     ]
    }
   ],
   "source": [
    "# Results for SVM model trained on word embeddings\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_2, Test_Y)*100)\n",
    "print(\"SVM Precision Score -> \",precision_score(predictions_SVM_2, Test_Y)*100)\n",
    "print(\"SVM Recall Score -> \",recall_score(predictions_SVM_2, Test_Y)*100)\n",
    "print(\"SVM F1 Score -> \",f1_score(predictions_SVM_2, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature function with tf-idf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_features(dataset,datatype):\n",
    "    Tfidf_vect_suffix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_suffix.fit([word[-3:] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]\n",
    "    +[word[-3:] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "    Tfidf_vect_prefix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_prefix.fit([word[:3] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]\n",
    "    +[word[:3] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "\n",
    "    # [(word) for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]\n",
    "    features = []\n",
    "    tokens = dataset[datatype][\"tokens\"]\n",
    "    pos_tags = dataset[datatype][\"pos_tags\"]\n",
    "    for sublist in range(len(tokens[:])):\n",
    "        for word_ind in range(len(tokens[sublist])):\n",
    "            word = tokens[sublist][word_ind]\n",
    "            # print(word)\n",
    "            vec = []\n",
    "            # vec.append(word)\n",
    "\n",
    "            #TF_IDF value of word\n",
    "            vec.append(Tfidf_vect.vocabulary_[word])\n",
    "\n",
    "            #Whether word is first word of sentence.\n",
    "            if word_ind==0:\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is last word of sentence.\n",
    "            if word_ind==(len(tokens[sublist])-1):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is title or has first letter capitalised.\n",
    "            \n",
    "            if word.istitle():\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            #TF_IDF vale of suffix of word.\n",
    "            vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]])\n",
    "\n",
    "            #TF_IDF vale of prefix of word.\n",
    "            vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]])\n",
    "\n",
    "\n",
    "            #POS tag of word.\n",
    "            vec.append(pos_tags[sublist][word_ind])\n",
    "\n",
    "            #POS tag of preceeding word. If first word, then non-exisitng pos_tag value 47 taken to indicate boundary.\n",
    "            if word_ind>0:\n",
    "                vec.append(pos_tags[sublist][word_ind-1])\n",
    "            else:\n",
    "                vec.append(47)\n",
    "            \n",
    "\n",
    "            #POS tag of suceeding word.\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                vec.append(pos_tags[sublist][word_ind+1])\n",
    "            else:\n",
    "                vec.append(47)\n",
    "\n",
    " \n",
    "            #Whether word contains any digit.\n",
    "            if bool(re.search(r'\\d', word)):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            features.append(np.array(vec))\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  92.4927317756003\n"
     ]
    }
   ],
   "source": [
    "# Classifier - SVM\n",
    "# Vectorizer - Tf-IDF\n",
    "# Kernel - rbf\n",
    "# Fit the training dataset on the classifier\n",
    "SVM_tf_idf_features_rbf = SVC(kernel='rbf')\n",
    "SVM_tf_idf_features_rbf.fit(tf_idf_features(dataset,\"train\"),[word for sublist in NEI_dataset[\"train\"][\"ner_tags\"]  for word in sublist])\n",
    "# Predict the labels on test dataset\n",
    "predictions_SVM_tf_idf_features_rbf = SVM_tf_idf_features_rbf.predict(tf_idf_features(dataset,\"test\"))\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_tf_idf_features_rbf, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature function with word-embedding vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_features(dataset,datatype):\n",
    "    Tfidf_vect_suffix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_suffix.fit([word[-3:] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "    Tfidf_vect_prefix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_prefix.fit([word[:3] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "\n",
    "    # [(word) for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]\n",
    "    features = []\n",
    "    tokens = dataset[datatype][\"tokens\"]\n",
    "    pos_tags = dataset[datatype][\"pos_tags\"]\n",
    "    for sublist in range(len(tokens[:])):\n",
    "        for word_ind in range(len(tokens[sublist])):\n",
    "            word = tokens[sublist][word_ind]\n",
    "            # print(word)\n",
    "            vec = []\n",
    "            # vec.append(word)\n",
    "\n",
    "            #word embedding vector of word\n",
    "            if word in model_w2v:\n",
    "                vec.extend(list((model_w2v.get_vector(word))))\n",
    "            else:\n",
    "                vec.extend(list((np.zeros(300))))\n",
    "            \n",
    "            #Whether word is first word of sentence.\n",
    "            if word_ind==0:\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is last word of sentence.\n",
    "            if word_ind==(len(tokens[sublist])-1):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is title or has first letter capitalised.\n",
    "            \n",
    "            if word.istitle():\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            #TF_IDF vale of suffix of word.\n",
    "            # vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]])\n",
    "            vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]]/Tfidf_vect_suffix.vocabulary_.__len__())\n",
    "\n",
    "\n",
    "            #TF_IDF vale of prefix of word.\n",
    "            # vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]])\n",
    "            vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]]/Tfidf_vect_prefix.vocabulary_.__len__())\n",
    "\n",
    "\n",
    "\n",
    "            #POS tag of word.\n",
    "            vec.append(pos_tags[sublist][word_ind]/47)\n",
    "\n",
    "            #POS tag of preceeding word. If first word, then non-exisitng pos_tag value 47 taken to indicate boundary.\n",
    "            if word_ind>0:\n",
    "                vec.append(pos_tags[sublist][word_ind-1]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "            \n",
    "\n",
    "            #POS tag of suceeding word.\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                vec.append(pos_tags[sublist][word_ind+1]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    " \n",
    "            #Whether word contains any digit.\n",
    "            if bool(re.search(r'\\d', word)):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            features.append(np.array(vec))\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  98.38699257025951\n"
     ]
    }
   ],
   "source": [
    "# Classifier - SVM\n",
    "# Vectorizer - Word Embedding\n",
    "# Kernel - rbf\n",
    "# Fit the training dataset on the classifier\n",
    "SVM_embedding_features_rbf = SVC(kernel='rbf')\n",
    "SVM_embedding_features_rbf.fit(embedding_features(dataset,\"train\"),[word for sublist in NEI_dataset[\"train\"][\"ner_tags\"]  for word in sublist])\n",
    "# Predict the labels on test dataset\n",
    "predictions_SVM_embedding_features_rbf = SVM_embedding_features_rbf.predict(embedding_features(dataset,\"test\"))\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_embedding_features_rbf, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_tf_idf_rbf\t93.56519866480025\t65.81607495069034\t96.12891609650703\t78.13551880579541\n",
      "SVM_embedding_rbf\t97.45235275115753\t90.53254437869822\t94.6513725995618\t92.5461533614769\n",
      "SVM_tf_idf_features_rbf\t92.4927317756003\t83.72781065088756\t75.82049564634963\t79.57820738137082\n",
      "SVM_embedding_features_rbf\t98.38699257025951\t95.67307692307693\t95.1219512195122\t95.39671808739475\n"
     ]
    }
   ],
   "source": [
    "# Results for SVM model on different input features\n",
    "models = [\"SVM_tf_idf_rbf\",\"SVM_embedding_rbf\",\"SVM_tf_idf_features_rbf\",\"SVM_embedding_features_rbf\"]\n",
    "predictions = [predictions_SVM_tf_idf_rbf,predictions_SVM_embedding_rbf,predictions_SVM_tf_idf_features_rbf,predictions_SVM_embedding_features_rbf]\n",
    "for i in range(len(models)):\n",
    "    print(models[i],accuracy_score(predictions[i], Test_Y)*100,precision_score(predictions[i], Test_Y)*100,recall_score(predictions[i], Test_Y)*100,f1_score(predictions[i], Test_Y)*100,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW Features added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Person_names.pkl','rb') as handle:\n",
    "    person_name = pickle.load(handle)\n",
    "with open('Location_names.pkl','rb') as handle:\n",
    "    location_name = pickle.load(handle)\n",
    "with open('Organization_names.pkl','rb') as handle:\n",
    "    organization_name = pickle.load(handle)\n",
    "with open('Country_Abb_CAPS.pkl','rb') as handle:\n",
    "    loc_ABBV = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(n):\n",
    "    try:\n",
    "        float(n)\n",
    "    except ValueError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def containsDigit(s):\n",
    "    num = [i for i in range(0,10)]\n",
    "    num = [str(i) for i in num]\n",
    "    for n in num:\n",
    "      if n in s:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "punc = set(punctuation)\n",
    "def containsSpecialChar(s):\n",
    "    for p in punc:\n",
    "      if p in s:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isPerson(word):\n",
    "    return word.lower() in person_name\n",
    "\n",
    "def isOrganization(word):\n",
    "    return word.lower() in organization_name\n",
    "\n",
    "def isLocation(word):\n",
    "    if word.lower() in location_name:\n",
    "        return True\n",
    "    return word in loc_ABBV\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_updated_features(dataset,datatype):\n",
    "    Tfidf_vect_suffix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_suffix.fit([word[-3:] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "    Tfidf_vect_prefix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_prefix.fit([word[:3] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "\n",
    "    # [(word) for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]\n",
    "    features = []\n",
    "    tokens = dataset[datatype][\"tokens\"]\n",
    "    pos_tags = dataset[datatype][\"pos_tags\"]\n",
    "    chunk_tags = dataset[datatype][\"chunk_tags\"]\n",
    "    for sublist in range(len(tokens[:])):\n",
    "        for word_ind in range(len(tokens[sublist])):\n",
    "            word = tokens[sublist][word_ind]\n",
    "            # print(word)\n",
    "            vec = []\n",
    "            # vec.append(word)\n",
    "\n",
    "            #word embedding vector of word\n",
    "            if word in model_w2v:\n",
    "                vec.extend(list((model_w2v.get_vector(word))))\n",
    "            else:\n",
    "                vec.extend(list((np.zeros(300))))\n",
    "            \n",
    "            #Whether word is first word of sentence.\n",
    "            if word_ind==0:\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is last word of sentence.\n",
    "            if word_ind==(len(tokens[sublist])-1):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is title or has first letter capitalised.\n",
    "            \n",
    "            if word.istitle():\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            #TF_IDF vale of suffix of word.\n",
    "            # vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]])\n",
    "            vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]]/Tfidf_vect_suffix.vocabulary_.__len__())\n",
    "\n",
    "\n",
    "            #TF_IDF vale of prefix of word.\n",
    "            # vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]])\n",
    "            vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]]/Tfidf_vect_prefix.vocabulary_.__len__())\n",
    "\n",
    "\n",
    "\n",
    "            #POS tag of word.\n",
    "            vec.append(pos_tags[sublist][word_ind]/47)\n",
    "\n",
    "            #POS tag of preceeding word. If first word, then non-exisitng pos_tag value 47 taken to indicate boundary.\n",
    "            if word_ind>0:\n",
    "                vec.append(pos_tags[sublist][word_ind-1]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "            \n",
    "\n",
    "            #POS tag of suceeding word.\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                vec.append(pos_tags[sublist][word_ind+1]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    " \n",
    "            #Whether word contains any digit.\n",
    "            if bool(re.search(r'\\d', word)):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "\n",
    "            '''\n",
    "            Newly added features\n",
    "            # next,prev Caps; index word location; word length, ancestor, chunk tag prev next curr\n",
    "            '''\n",
    "\n",
    "            # word location normalised\n",
    "            vec.append(word_ind/len(tokens[sublist]))\n",
    "\n",
    "            # word length\n",
    "            vec.append(len(word)/20)\n",
    "\n",
    "            # prev to prev pos\n",
    "            if word_ind>1:\n",
    "                vec.append(pos_tags[sublist][word_ind-2]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    "            # next to next pos\n",
    "            if word_ind < (len(tokens[sublist])-2):\n",
    "                vec.append(pos_tags[sublist][word_ind+2]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    "            # chunk tag of word all normalised\n",
    "            vec.append(chunk_tags[sublist][word_ind]/22)\n",
    "\n",
    "            # chunk tag of preceeding word. If first word, then non-exisitng chunk_tag value 0 taken to outside chunk.\n",
    "            if word_ind>0:\n",
    "                vec.append(chunk_tags[sublist][word_ind-1]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            # chunk tag of next word\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                vec.append(chunk_tags[sublist][word_ind+1]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # prev to prev chunk\n",
    "            if word_ind>1:\n",
    "                vec.append(chunk_tags[sublist][word_ind-2]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            # chunk tag of next to next word\n",
    "            if word_ind < (len(tokens[sublist])-2):\n",
    "                vec.append(chunk_tags[sublist][word_ind+2]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "            # if previous word is capital word\n",
    "            if word_ind>0:\n",
    "                next_word = tokens[sublist][word_ind-1]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # if next word is capital word\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                next_word = tokens[sublist][word_ind+1]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # if previous to previous word is capital word\n",
    "            if word_ind>1:\n",
    "                next_word = tokens[sublist][word_ind-2]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # if next to next word is capital word\n",
    "            if word_ind < (len(tokens[sublist])-2):\n",
    "                next_word = tokens[sublist][word_ind+2]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "\n",
    "            c = 1 #100\n",
    "            # is number\n",
    "            if is_number(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            # containsDigit\n",
    "            if containsDigit(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # contains special characters\n",
    "            if containsSpecialChar(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "            '''\n",
    "            Gazetteer List\n",
    "            '''\n",
    "            c = 1 # 400\n",
    "            if isPerson(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            if isOrganization(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            if isLocation(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            features.append(np.array(vec))\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier - SVM\n",
    "# Vectorizer - Word Embedding\n",
    "# Kernel - rbf\n",
    "# Fit the training dataset on the classifier\n",
    "SVM_embedding_updated_features_rbf = SVC(kernel='rbf')\n",
    "SVM_embedding_updated_features_rbf.fit(embedding_updated_features(dataset,\"train\"),[word for sublist in NEI_dataset[\"train\"][\"ner_tags\"]  for word in sublist])\n",
    "# Predict the labels on test dataset\n",
    "predictions_SVM_embedding_updated_features_rbf = SVM_embedding_updated_features_rbf.predict(embedding_updated_features(dataset,\"test\"))\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM_embedding_updated_features_rbf, Test_Y)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store models as pickle object to be reused later without retraining:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\"SVM_tf_idf_rbf\",\"SVM_embedding_rbf\",\"SVM_tf_idf_features_rbf\",\"SVM_embedding_features_rbf\",\"SVM_embedding_updated_features_rbf\"]\n",
    "# actual_models = [SVM_tf_idf_rbf,SVM_embedding_rbf,SVM_tf_idf_features_rbf,SVM_embedding_features_rbf,SVM_embedding_updated_features_rbf]\n",
    "\n",
    "# for i in range(len(models)):\n",
    "#     pickle.dump(actual_models[i], open(models[i]+\".pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models stored as pickle objects as training SVM models is slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM_tf_idf_rbf\t93.56519866480025\t65.81607495069034\t96.12891609650703\t78.13551880579541\n",
      "SVM_embedding_rbf\t97.45235275115753\t90.53254437869822\t94.6513725995618\t92.5461533614769\n",
      "SVM_tf_idf_features_rbf\t92.4927317756003\t83.72781065088756\t75.82049564634963\t79.57820738137082\n",
      "SVM_embedding_features_rbf\t98.38699257025951\t95.67307692307693\t95.1219512195122\t95.39671808739475\n",
      "SVM_embedding_updated_features_rbf\t98.52051254441693\t96.30177514792899\t95.27991218441272\t95.78811844767336\n"
     ]
    }
   ],
   "source": [
    "models = [\"SVM_tf_idf_rbf\",\"SVM_embedding_rbf\",\"SVM_tf_idf_features_rbf\",\"SVM_embedding_features_rbf\",\"SVM_embedding_updated_features_rbf\"]\n",
    "predictions_str = [\"predictions_SVM_tf_idf_rbf\",\"predictions_SVM_embedding_rbf\",\"predictions_SVM_tf_idf_features_rbf\",\"predictions_SVM_embedding_features_rbf\",\"predictions_SVM_embedding_updated_features_rbf\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    exec(models[i] + \" = \" + 'pickle.load(open(\"' + models[i] + '.pkl\", \"rb\"))')\n",
    "\n",
    "actual_models = [SVM_tf_idf_rbf,SVM_embedding_rbf,SVM_tf_idf_features_rbf,SVM_embedding_features_rbf,SVM_embedding_updated_features_rbf]\n",
    "\n",
    "predictions_SVM_tf_idf_rbf = SVM_tf_idf_rbf.predict(Test_X_Tfidf)\n",
    "predictions_SVM_embedding_rbf = SVM_embedding_rbf.predict(test_embeddings)\n",
    "predictions_SVM_tf_idf_features_rbf = SVM_tf_idf_features_rbf.predict(tf_idf_features(dataset,\"test\"))\n",
    "predictions_SVM_embedding_features_rbf = SVM_embedding_features_rbf.predict(embedding_features(dataset,\"test\"))\n",
    "predictions_SVM_embedding_updated_features_rbf = SVM_embedding_updated_features_rbf.predict(embedding_updated_features(dataset,\"test\"))\n",
    "predictions = [predictions_SVM_tf_idf_rbf,predictions_SVM_embedding_rbf,predictions_SVM_tf_idf_features_rbf,predictions_SVM_embedding_features_rbf,predictions_SVM_embedding_updated_features_rbf]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    print(models[i],accuracy_score(predictions[i], Test_Y)*100,precision_score(predictions[i], Test_Y)*100,recall_score(predictions[i], Test_Y)*100,f1_score(predictions[i], Test_Y)*100,sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate chunk and pos tags for unseen sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tokenizer = {}\n",
    "chunk_tokenizer = {}\n",
    "chunk_max_token = {}\n",
    "pos_max_token = {}\n",
    "def generate_chunk_pos_tags():\n",
    "    for sent,pos,chunks in zip(\n",
    "        dataset['train']['tokens']+dataset['validation']['tokens']+dataset['test']['tokens'], \n",
    "        dataset['train']['pos_tags']+dataset['validation']['pos_tags']+dataset['test']['pos_tags'], \n",
    "        dataset['train']['chunk_tags']+dataset['validation']['chunk_tags']+dataset['test']['chunk_tags']):\n",
    "        tree = ne_chunk(pos_tag(word_tokenize(\" \".join(sent))))\n",
    "        iob_tags = tree2conlltags(tree)\n",
    "        for i in range(len(iob_tags)):\n",
    "            chunk_pos_id = iob_tags[i][1]+'_'+iob_tags[i][2]\n",
    "\n",
    "            try:\n",
    "                if chunk_pos_id in pos_tokenizer.keys():\n",
    "                    if pos[i] in pos_tokenizer[chunk_pos_id].keys():\n",
    "                        pos_tokenizer[chunk_pos_id][pos[i]] += 1 \n",
    "                    else:\n",
    "                        pos_tokenizer[chunk_pos_id][pos[i]] = 1\n",
    "                else: \n",
    "                    pos_tokenizer[chunk_pos_id] = {pos[i]:1}\n",
    "            except Exception as e:\n",
    "                continue  \n",
    "            \n",
    "            if chunk_pos_id in chunk_tokenizer.keys():\n",
    "                if chunks[i] in chunk_tokenizer[chunk_pos_id].keys():\n",
    "                    chunk_tokenizer[chunk_pos_id][chunks[i]] += 1 \n",
    "                else:\n",
    "                    chunk_tokenizer[chunk_pos_id][chunks[i]] = 1\n",
    "            else: \n",
    "                chunk_tokenizer[chunk_pos_id] = {chunks[i]:1}\n",
    "\n",
    "\n",
    "    for k,v in chunk_tokenizer.items():\n",
    "        chunk_max_token[k] = max(v, key=v.get)\n",
    "    for k,v in pos_tokenizer.items():\n",
    "        pos_max_token[k] = max(v, key=v.get)\n",
    "\n",
    "generate_chunk_pos_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(sent_list, pass1_prediction = None):\n",
    "    sent_list_pos = []\n",
    "    sent_list_chunk = []\n",
    "    tree = ne_chunk(pos_tag(word_tokenize(\" \".join(sent_list))))\n",
    "    iob_tags = tree2conlltags(tree)\n",
    "    prev_prediction = pass1_prediction\n",
    "    pred_ind = 0\n",
    "    for w,p,c in iob_tags:\n",
    "        key_id = p+'_'+c\n",
    "        if key_id in pos_max_token.keys():\n",
    "            sent_list_pos.append(pos_max_token[key_id])\n",
    "        else:\n",
    "            sent_list_pos.append(0)\n",
    "\n",
    "        if key_id in chunk_max_token.keys():\n",
    "            sent_list_chunk.append(chunk_max_token[key_id])\n",
    "        else:\n",
    "            sent_list_chunk.append(0)\n",
    "\n",
    "\n",
    "\n",
    "    Tfidf_vect_suffix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_suffix.fit([word[-3:] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word[-3:] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "    Tfidf_vect_prefix = TfidfVectorizer(lowercase=False,token_pattern=r\".*\")\n",
    "    Tfidf_vect_prefix.fit([word[:3] for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"validation\"][\"tokens\"]  for word in sublist]+[word[:3] for sublist in dataset[\"test\"][\"tokens\"]  for word in sublist])\n",
    "\n",
    "    # [(word) for sublist in dataset[\"train\"][\"tokens\"]  for word in sublist]\n",
    "    features = []\n",
    "    # print(sent_list)\n",
    "    tokens = []\n",
    "    tokens.append(list(sent_list.split(\" \")))\n",
    "    # print(tokens)\n",
    "    pos_tags = [sent_list_pos]\n",
    "    chunk_tags = [sent_list_chunk]\n",
    "    for sublist in range(len(tokens[:])):\n",
    "        for word_ind in range(len(tokens[sublist])):\n",
    "            word = tokens[sublist][word_ind]\n",
    "            # print(word)\n",
    "            vec = []\n",
    "            # vec.append(word)\n",
    "\n",
    "            #word embedding vector of word\n",
    "            if word in model_w2v:\n",
    "                vec.extend(list((model_w2v.get_vector(word))))\n",
    "            else:\n",
    "                vec.extend(list((np.zeros(300))))\n",
    "            \n",
    "            #Whether word is first word of sentence.\n",
    "            if word_ind==0:\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is last word of sentence.\n",
    "            if word_ind==(len(tokens[sublist])-1):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            #Whether word is title or has first letter capitalised.\n",
    "            \n",
    "            if word.istitle():\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            #TF_IDF vale of suffix of word.\n",
    "            # vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]])\n",
    "            if word[-3:] in Tfidf_vect_suffix.vocabulary_:\n",
    "                vec.append(Tfidf_vect_suffix.vocabulary_[word[-3:]]/Tfidf_vect_suffix.vocabulary_.__len__())\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "            #TF_IDF vale of prefix of word.\n",
    "            # vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]])\n",
    "            if word[:3] in Tfidf_vect_prefix.vocabulary_:\n",
    "                vec.append(Tfidf_vect_prefix.vocabulary_[word[:3]]/Tfidf_vect_prefix.vocabulary_.__len__())\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "            #POS tag of word.\n",
    "            vec.append(pos_tags[sublist][word_ind]/47)\n",
    "\n",
    "            #POS tag of preceeding word. If first word, then non-exisitng pos_tag value 47 taken to indicate boundary.\n",
    "            if word_ind>0:\n",
    "                vec.append(pos_tags[sublist][word_ind-1]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "            \n",
    "\n",
    "            #POS tag of suceeding word.\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                vec.append(pos_tags[sublist][word_ind+1]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    "\n",
    "            #Whether word contains any digit.\n",
    "            if bool(re.search(r'\\d', word)):\n",
    "                vec.append(1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "\n",
    "            '''\n",
    "            Newly added features\n",
    "            # next,prev Caps; index word location; word length, ancestor, chunk tag prev next curr\n",
    "            '''\n",
    "\n",
    "            # word location normalised\n",
    "            vec.append(word_ind/len(tokens[sublist]))\n",
    "\n",
    "            # word length\n",
    "            vec.append(len(word)/20)\n",
    "\n",
    "            # prev to prev pos\n",
    "            if word_ind>1:\n",
    "                vec.append(pos_tags[sublist][word_ind-2]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    "            # next to next pos\n",
    "            if word_ind < (len(tokens[sublist])-2):\n",
    "                vec.append(pos_tags[sublist][word_ind+2]/47)\n",
    "            else:\n",
    "                vec.append(47/47)\n",
    "\n",
    "            # chunk tag of word all normalised\n",
    "            vec.append(chunk_tags[sublist][word_ind]/22)\n",
    "\n",
    "            # chunk tag of preceeding word. If first word, then non-exisitng chunk_tag value 0 taken to outside chunk.\n",
    "            if word_ind>0:\n",
    "                vec.append(chunk_tags[sublist][word_ind-1]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            # chunk tag of next word\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                vec.append(chunk_tags[sublist][word_ind+1]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # prev to prev chunk\n",
    "            if word_ind>1:\n",
    "                vec.append(chunk_tags[sublist][word_ind-2]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            # chunk tag of next to next word\n",
    "            if word_ind < (len(tokens[sublist])-2):\n",
    "                vec.append(chunk_tags[sublist][word_ind+2]/22)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "            # if previous word is capital word\n",
    "            if word_ind>0:\n",
    "                next_word = tokens[sublist][word_ind-1]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # if next word is capital word\n",
    "            if word_ind < (len(tokens[sublist])-1):\n",
    "                next_word = tokens[sublist][word_ind+1]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # if previous to previous word is capital word\n",
    "            if word_ind>1:\n",
    "                next_word = tokens[sublist][word_ind-2]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # if next to next word is capital word\n",
    "            if word_ind < (len(tokens[sublist])-2):\n",
    "                next_word = tokens[sublist][word_ind+2]\n",
    "                if next_word.istitle():\n",
    "                    vec.append(1)\n",
    "                else:\n",
    "                    vec.append(0)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "\n",
    "            c = 1 #100\n",
    "            # is number\n",
    "            if is_number(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "            \n",
    "            # containsDigit\n",
    "            if containsDigit(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            # contains special characters\n",
    "            if containsSpecialChar(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "\n",
    "            '''\n",
    "            Gazetteer List\n",
    "            '''\n",
    "            c = 1 # 400\n",
    "            if isPerson(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            if isOrganization(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            if isLocation(word):\n",
    "                vec.append(c*1)\n",
    "            else:\n",
    "                vec.append(0)\n",
    "\n",
    "            features.append(np.array(vec))\n",
    "    features = np.array(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    return SVM_embedding_updated_features_rbf.predict(make_features(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_embedding_updated_features_rbf.predict(make_features('The State Bank of India is the largest bank in the country'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 1 0 0 0 0 0 0 0]\n",
      "[1 1 0 0 0]\n",
      "[1 1 0 0 1]\n",
      "[1 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0]\n",
      "[1 0 0 0 0 0]\n",
      "[1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentence('The State Bank of India is the largest bank in the country'))\n",
    "\n",
    "print(predict_sentence('Pushpak Bhattacharyya teaches us CS626'))\n",
    "\n",
    "print(predict_sentence('Pushpak Bhattacharyya teaches us CS'))\n",
    "\n",
    "print(predict_sentence('India got its freedom on 15th August 1947'))\n",
    "\n",
    "print(predict_sentence('India got its freedom on 15-8-1947'))\n",
    "\n",
    "print(predict_sentence('India got its freedom on 15/8/1947'))\n",
    "\n",
    "print(predict_sentence('India got its freedom on 15.8.1947'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n",
      "[1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentence('Schools reopened after Covid.'))\n",
    "print(predict_sentence('Delhi reopened after Covid.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentence('india has won the match against afganistan.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
